{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/yandexdataschool/mlhep2019/master/notebooks/day-5/tracking/metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/yandexdataschool/mlhep2019/master/notebooks/day-5/tracking/user_test_submission.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/yandexdataschool/mlhep2019/master/notebooks/day-5/tracking/public_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "import user_test_submission as submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from metrics import predictor\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Pattern Recognition using Linear Approximation of a Track\n",
    "Original competition: https://ramp.studio/problems/HEP_tracking\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Track pattern recognition is an early step of the reconstruction of data coming from a particle detector. It recognizes tracks among the subdetectors hits. Reconstructed track parameters allow to estimate the particle deviation in a magnetic field, and thus reconstruct its charge and momentum. This information is used for the reconstruction of the decay vertex, to identify the mother particle and for further particle identification.\n",
    "\n",
    "There is wide variety of the track pattern recognition methods. They differ in how they process the hits, what kind of tracks they are able to recognize and which requirements these tracks should satisfy. Therefore, specifics of an experiment and the detector geometry affect the tracking performance and track pattern recognition methods should be adapted to it accordingly.\n",
    "\n",
    "In this notebook a track pattern recognition for a 2D detector with circular geometry and uniform magnetic field is considered. The detector schema with hits and tracks of an event is shown in the figure below. The challenge is to recognize tracks of an event with the highest efficiecny. It supposed that one hit can belong to only one track. \n",
    "\n",
    "<img src=\"pic/detector.png\" /> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Quick Glossary\n",
    "\n",
    "We use the following vocabulary in the context of practice session. It may be slightly different from the general high-energy physics context. Some terms are not used in the workbook, but kept here in case they are used in the discussions.\n",
    "\n",
    "* **event**: a recorded (or simulated) collision in which many particles are present, the basic unit of particle physics data\n",
    "* **pixel**: the smallest detector unit\n",
    "* **hit**: a pixel through which a particle has passed and left signal in a given event\n",
    "* **cluster**: a set of hits, belonging (or predicted to be belonging) to the trajectory of a single particle\n",
    "* **reconstruct**: same thing as predict, but may also refer to further derived quantities\n",
    "* **track**: a reconstructed particle, that is, a cluster but also including additional derived information, such as the overall curvature and angle\n",
    "* **vertex**: the point close to the center of the detector, from which the particles have originated\n",
    "* **impact parameter**: the shortest distance between a track and the origin\n",
    "* **(momentum/angle/etc) resolution** : width of the normal distribution of the difference between a predicted quantity and the true value of that quantity\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The main objective of the challenge is the optimal matching of the hits in an event. The positions of the hits in the detector are provided as input data, and the user is expected to implement a clustering scheme (trained on data if needed), so that every hit is assigned to a cluster id.\n",
    "\n",
    "The value of the cluster id itself is not relevant for the task, what is relevant is which hits are clustered together, and whether this clustering corresponds well to the input particles. The score function that describes this is included in the notebook, and details will be mentioned there.\n",
    "\n",
    "## Application\n",
    "\n",
    "The user is expected to implement the class `clusterer.py`, which contains the `__init__`, `fit`, and `predict_single_event` functions.\n",
    "\n",
    "* **`__init__`** is where parameters should be set.\n",
    "* **`fit`** is the training function (not to be confused with track-fitting), where the algorithm has access to the ground-truth. This function is to be run once on an input array that contains a set of training events. The user is able to implement any event-level or particle-level segmentation of the input array in order to set up the training in any desired way.\n",
    "* **`predict_single_event`** is the function to reconstruct the hit clusters (tracks), returning an array of predicted (reconstructed) ids associated to each hit in the input array. This function takes only the hits from a single event as input, with the event_id dropped, and the RAMP machinery takes care of running this function on each event.\n",
    "\n",
    "The **task**  is to implement this class in a way that the predict_single_event function returns a numpy array of assigned cluster ids. At any level of this task, machine-learning techniques can be employed for sub-tasks defined by the user.\n",
    "\n",
    "## Detector\n",
    "\n",
    "Image from the Atlas experiment:\n",
    "\n",
    "<img src=\"pic/atlas2009.png\" /> <br>\n",
    "\n",
    "The data provided to the user is a list of hit positions from a simple toy detector model that mimics the Atlas detector design (which is generic enough for recent silicon-based tracking detectors). The detector has an onion-like geometry with 9 layers surrounding the origin with polar distances of $R = [39,85,155,213,271,405,562,762,1000]$ cm. These layers have a very small thicknesses compared to the distances, therefore the thickness can be neglected.\n",
    "\n",
    "Each layer is segmented in azimuth with high granularity. There are ($2\\pi$R/pitch)+1 pixels in every layer, where pitch is 0.025 cm for layers 0-4 and 0.05 cm for layers 5-9.\n",
    "\n",
    "Every \"pixel\" corresponds to the smallest detector unit defined by `layer` and `iphi` (azimuthal index).\n",
    "\n",
    "## Simulation\n",
    "\n",
    "The task uses a toy model for particle generation and simulation, in which a Poisson distribution is sampled to determine the number of particles in each event, with an average of 10 particles per event.\n",
    "\n",
    "The particles are sampled uniformly in azimuth and momentum with bounds on the momentum. Each particle originates from a vertex that is also randomly sampled from a narrow normal distribution around the origin. The proper dimensions of the momentum and position and determination of these values for the tracks are beyond the scope of the challenge.\n",
    "\n",
    "The particles generated this way are simulated in a uniform magnetic field. The detector material is considered to cause multiple-scattering, and this is implemented as a random rotation of the particle momentum at every detector position, sampled from a narrow normal distribution that roughly corresponds to the material of the Atlas tracker.\n",
    "\n",
    "In addition, some hit inefficiency is simulated by a random drop of some hits (with 3% probability), and a particle stopping probability of 1% is applied at each layer to simulate effects of hadronic interactions. Keeping these in mind, the algorithms might be desired to be able to handle cases when the particle doesn't have a hit on every layer.\n",
    "\n",
    "Since the detector has a very high granularity in azimuth, the cases where two particles pass through a single pixel are neglected (less than 0.2% probability).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "This notebook demonstrate how linear approximation of a track can be used for track pattern recognition. The notebook describes input data, the track pattern recognition method and qualyti metrics, and shows how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo pip install sklearn==0.18.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('public_train.csv', index_col=False)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['cluster_id'], axis=1).values\n",
    "y = data[['event_id', 'cluster_id']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the hits in a single event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get event ids\n",
    "event_ids = X[:, 0]\n",
    "\n",
    "# Estimate unique event ids\n",
    "unique_event_ids = np.unique(event_ids)\n",
    "\n",
    "# Choose an event\n",
    "i_event = unique_event_ids[0]\n",
    "\n",
    "X_event = X[event_ids == i_event]\n",
    "y_event = y[event_ids == i_event]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event(X, y=None, is_plot=True):\n",
    "    \n",
    "    cmap = ['k', 'b', 'y', 'g', 'r', 'k']\n",
    "    R=[39, 85, 155, 213, 271, 405, 562, 762, 1000]\n",
    "    \n",
    "    hit_x = X[:, 3]\n",
    "    hit_y = X[:, 4]\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    if y is None:\n",
    "        plt.scatter(hit_x, hit_y)\n",
    "        \n",
    "    if y is not None:\n",
    "        for i_track in np.unique(y):\n",
    "            track_mask = y == i_track\n",
    "            track_x = hit_x[track_mask]\n",
    "            track_y = hit_y[track_mask]\n",
    "            track_r = np.sqrt(track_x**2 + track_y**2)\n",
    "            sorted_ids = np.argsort(track_r)\n",
    "            if is_plot:\n",
    "                plt.plot(track_x[sorted_ids], track_y[sorted_ids], color=cmap[int(i_track) % len(cmap)])\n",
    "            plt.scatter(track_x[sorted_ids], track_y[sorted_ids], color=cmap[int(i_track) % len(cmap)])\n",
    "        \n",
    "    for ar in R:\n",
    "        circle=plt.Circle((0,0), ar, color='0.5', fill=False)\n",
    "        plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    plt.xlim(-1100, 1100)\n",
    "    plt.ylim(-1100, 1100)\n",
    "    plt.xticks(size=12)\n",
    "    plt.yticks(size=12)\n",
    "    plt.xlabel('X', size=12)\n",
    "    plt.ylabel('Y', size=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You have this:\")\n",
    "plot_event(X_event, None)\n",
    "\n",
    "print(\"You need to get this:\")\n",
    "plot_event(X_event, y_event[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "event_ids = numpy.unique(data['event_id'].values)\n",
    "\n",
    "event_ids_train, event_ids_test = train_test_split(event_ids, \n",
    "                                                   test_size=1000, \n",
    "                                                   random_state=42)\n",
    "\n",
    "X_train, y_train = X[data['event_id'].isin(event_ids_train)], y[data['event_id'].isin(event_ids_train)]\n",
    "X_test, y_test   = X[data['event_id'].isin(event_ids_test)],  y[data['event_id'].isin(event_ids_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Pattern Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering based approach\n",
    "\n",
    "This method is based on the clustering methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clusterer(object):\n",
    "\n",
    "    def __init__(self, cluster=None):\n",
    "\n",
    "        self.cluster = cluster\n",
    "\n",
    "    def get_polar(self, x, y):\n",
    "\n",
    "        x = numpy.array(x)\n",
    "        y = numpy.array(y)\n",
    "\n",
    "        phi = numpy.arctan2(y, x)\n",
    "        r = numpy.sqrt(x**2 + y**2)\n",
    "\n",
    "        return r, phi\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def predict_single_event(self, X):\n",
    "\n",
    "        x, y = X[:, 3], X[:, 4]\n",
    "        r, phi = self.get_polar(x, y)\n",
    "\n",
    "        self.cluster.fit(phi.reshape(-1, 1))\n",
    "        labels = self.cluster.labels_\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ctr = Clusterer(cluster=KMeans(n_clusters=5, n_init=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on single event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_event_pred = ctr.predict_single_event(X_event)\n",
    "y_event_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recognized tracks:\")\n",
    "plot_event(X_event, y_event_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on several events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from metrics import predictor\n",
    "y_pred_test = predictor(ctr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = submission.score_function(y_test, y_pred_test)\n",
    "print(\"Score: {:1.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Try to improve the score of track pattern recognition using clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code is here ###\n",
    "\n",
    "score = submission.score_function(y_test, y_pred_test)\n",
    "print(\"Score: {:1.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN based tracks pattern recognition\n",
    "\n",
    "Original article: https://www.epj-conferences.org/articles/epjconf/pdf/2017/19/epjconf_ctdw2017_00003.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a basic LSTM model architecture used to classify hits for one track. The LSTM and a fully-connected layer with a softmax activation read the pixel arrays and predict which pixels belong to the target track.\n",
    "\n",
    "<img src=\"pic/model.png\" width=\"600\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_phi_bin = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing functions\n",
    "\n",
    "class det_geo():\n",
    "    layer_r     = np.array([39,    85,    155,   213,   271,   405,  562,  762,  1000])\n",
    "    layer_pitch = np.array([0.025, 0.025, 0.025, 0.025, 0.025, 0.05, 0.05, 0.05, 0.05])\n",
    "    num_layers = layer_r.shape[0]\n",
    "    max_phi = (2 * np.pi * layer_r / layer_pitch + 1).astype(np.int)\n",
    "\n",
    "    \n",
    "def rescale_phi(phi, nbin, layer):\n",
    "    return (phi * float(nbin) / det_geo.max_phi[layer.astype(np.int)]).astype(np.int)\n",
    "\n",
    "\n",
    "def event_hits_transform(X, y=None, num_phi_bin=100):\n",
    "    \n",
    "    evids, layers, phis = X[:,0].astype(np.int), X[:,1].astype(np.int), X[:,2].astype(np.int)\n",
    "    phis = rescale_phi(phis, num_phi_bin, layers)\n",
    "    \n",
    "    # Count the number of first-layer hits, which will be my seeds\n",
    "    seed_idx = np.where(layers == 0)[0]\n",
    "    num_seeds = seed_idx.size\n",
    "\n",
    "    # Training input will contain the seed hit followed by the rest of the event\n",
    "    train_input = np.zeros((num_seeds, det_geo.num_layers, num_phi_bin))\n",
    "    # Training targets will be images of the individual signal tracks\n",
    "    train_target = np.zeros_like(train_input)\n",
    "\n",
    "    # Loop over training samples to prepare (seeds)\n",
    "    for i_sample in range(num_seeds):\n",
    "        \n",
    "        i_hit = seed_idx[i_sample]\n",
    "\n",
    "        # Fill the input first layer with just the seed hit\n",
    "        train_input[i_sample, 0, phis[i_hit]] = 1\n",
    "        # Fill the other layers with all remaining event hits\n",
    "        train_input[i_sample, layers[layers > 0], phis[layers > 0]] = 1\n",
    "\n",
    "        # Fill target with hits from this track\n",
    "        if y is not None:\n",
    "            track_ids = y[:, 1]\n",
    "            seed_track_id = y[i_hit, 1]\n",
    "            train_target[i_sample, layers[track_ids == seed_track_id], phis[track_ids == seed_track_id]] = 1\n",
    "        \n",
    "    return train_input, train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization function\n",
    "\n",
    "def plot_event_polar(X):\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    plt.pcolor(X.T)\n",
    "    \n",
    "    plt.xticks(size=12)\n",
    "    plt.yticks(size=12)\n",
    "    \n",
    "    plt.xlabel('Layer bins', size=12)\n",
    "    plt.ylabel('Phi bins', size=12)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess one event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X_event_polar, y_event_polar = event_hits_transform(X_event, y_event, num_phi_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shapes\n",
    "X_event_polar.shape, y_event_polar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_train = 0\n",
    "\n",
    "print(\"Original event hits:\")\n",
    "plot_event(X_event, None)\n",
    "\n",
    "print(\"RNN inputs for one track of the event:\")\n",
    "plot_event_polar(X_event_polar[i_train])\n",
    "\n",
    "print(\"RNN outputs for the track:\")\n",
    "plot_event_polar(y_event_polar[i_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all training events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train_input = []\n",
    "train_target = []\n",
    "\n",
    "event_ids = X_train[:, 0]\n",
    "for i_event in np.unique(event_ids):\n",
    "    \n",
    "    # Take one event\n",
    "    X_ev = X_train[event_ids == i_event]\n",
    "    y_ev = y_train[event_ids == i_event]\n",
    "\n",
    "    # Preprocess this event\n",
    "    train_input_ev, train_target_ev = event_hits_transform(X_ev, y_ev, num_phi_bin)\n",
    "    \n",
    "    # Store the preprocessed events\n",
    "    train_input.append(train_input_ev)\n",
    "    train_target.append(train_target_ev)\n",
    "\n",
    "X_train_polar = np.concatenate(tuple(train_input))\n",
    "y_train_polar = np.concatenate(tuple(train_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shapes\n",
    "X_train_polar.shape, y_train_polar.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Estimate device to train a model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a model\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(100, 50, batch_first=True)\n",
    "        self.dense1 = nn.Linear(50, 100)\n",
    "        self.softmax = nn.Softmax()\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        hid_sequence, (c_last, h_last) = self.lstm1(x)\n",
    "        hid_sequence = self.dense1(hid_sequence)\n",
    "        logits = self.softmax(hid_sequence)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassifier(object):\n",
    "    \n",
    "    def __init__(self, model, n_epochs=10, batch_size=64):\n",
    "        \"\"\"\n",
    "        Class to fit a pytorch model and to use it to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model: object\n",
    "            PyTorch model.\n",
    "        n_epochs: int\n",
    "            Number of epochs to fit the model.\n",
    "        batch_size: int\n",
    "            The batch size used for the model fitting.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = model\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the PyTorch model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.array\n",
    "            The model imput data.\n",
    "        y: numpy.array\n",
    "            Targets to predict.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert X and y into torch tensors\n",
    "        X_tensor = torch.as_tensor(X, dtype=torch.float32, device=device)\n",
    "        y_tensor = torch.as_tensor(y, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Create dataset for trainig procedure\n",
    "        train_data = TensorDataset(X_tensor, y_tensor)\n",
    "        \n",
    "        # Estimate optimizer\n",
    "        opt = torch.optim.Adam(self.model.parameters())\n",
    "        \n",
    "        # Enable droout\n",
    "        self.model.train(True)\n",
    "        \n",
    "        # Start the model fit\n",
    "        for epoch_i in range(self.n_epochs):\n",
    "            loss_history = []\n",
    "            for x_batch, y_batch in tqdm_notebook(DataLoader(train_data, batch_size=self.batch_size, shuffle=True)):\n",
    "                # make prediction on a batch\n",
    "                logits = self.model(x_batch)\n",
    "                # calculate loss\n",
    "                loss = -(torch.log(logits) * y_batch).sum(-1).mean()\n",
    "                # set gradients to zero\n",
    "                opt.zero_grad()\n",
    "                # backpropagate gradients\n",
    "                loss.backward()\n",
    "                # update the model weights\n",
    "                opt.step()\n",
    "                loss_history.append(loss.item())\n",
    "            print(\"epoch: %i, mean loss: %.5f\" % (epoch_i, np.mean(loss_history)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.array\n",
    "            The model imput data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Disable droout\n",
    "        self.model.train(False)\n",
    "        \n",
    "        # Convert X and y into torch tensors\n",
    "        X_tensor = torch.as_tensor(X, dtype=torch.float32, device=device)\n",
    "        # Make predictions for X \n",
    "        y_pred = model(X_tensor)\n",
    "        \n",
    "        return y_pred.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pytorch model\n",
    "model=Model().to(device=device)\n",
    "\n",
    "# Fit the model\n",
    "clf = MyClassifier(model=model, n_epochs=50, batch_size=64)\n",
    "clf.fit(X_train_polar, y_train_polar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for one event\n",
    "y_event_polar_pred = clf.predict(X_event_polar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RNN output for the track:\")\n",
    "plot_event_polar(y_event_polar_pred[i_train])\n",
    "\n",
    "print(\"True output for the track:\")\n",
    "plot_event_polar(y_event_polar[i_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test track pattern recognition algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clusterer(object):\n",
    "\n",
    "    def __init__(self, classifier=None):\n",
    "\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def predict_single_event(self, X):\n",
    "\n",
    "        trkid = np.zeros(X.shape[0])\n",
    "        for i in range(trkid.shape[0]):\n",
    "            \n",
    "            lays, phis = X[:,1].astype(np.int), X[:,2].astype(np.int)\n",
    "            phis = rescale_phi(phis, num_phi_bin, lays)\n",
    "            lay, phi = lays[i], phis[i]\n",
    "            \n",
    "            # Preprocess the event\n",
    "            X_polar, _ = event_hits_transform(X, None, num_phi_bin)\n",
    "            # Get predictions for the event\n",
    "            y_polar_pred = clf.predict(X_polar)\n",
    "\n",
    "            # Get the list of track scores\n",
    "            scores = y_polar_pred[:, lays[i], phis[i]]\n",
    "\n",
    "            # Select the best score\n",
    "            best = np.argmax(scores)\n",
    "            trkid[i] = best\n",
    "            \n",
    "        return trkid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = Clusterer(classifier=clf)\n",
    "\n",
    "y_event_pred = ctr.predict_single_event(X_event)\n",
    "y_event_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recognized tracks:\")\n",
    "plot_event(X_event, y_event_pred, is_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from metrics import predictor\n",
    "y_pred_test = predictor(ctr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = submission.score_function(y_test, y_pred_test)\n",
    "print(\"Score: {:1.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Create a deep model with the following layers:\n",
    "* Input layer\n",
    "* LSTM with 200 hidden neurons\n",
    "* 30% dropout\n",
    "* LSTM with 150 hidden neurons\n",
    "* 20% dropout\n",
    "* Output dense layer with 100 neurons and Softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Your code is here ###\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ### Your code is here ###\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pytorch model\n",
    "### Your code is here ###\n",
    "\n",
    "# Fit the model\n",
    "### Your code is here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate the quality score\n",
    "### Your code is here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (for the Jedi)\n",
    "\n",
    "Try to improve the score. Fill free to change anything you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code is here ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
