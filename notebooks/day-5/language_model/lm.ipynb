{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYGGdoMgjssw"
   },
   "source": [
    "# Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQligDFNjss0"
   },
   "source": [
    "In this tutorial we will use two character level language models to create dinosaur names. We will recurrent neural networks as main tool for language modellig. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8egeope4jss2"
   },
   "source": [
    "![picture](https://vignette.wikia.nocookie.net/uncyclopedia/images/7/73/Hankk_the_dino.png/revision/latest?cb=20100127020302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-05 16:05:42--  https://raw.githubusercontent.com/artemovae/NLP-seminar-LM/master/dinos.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19909 (19K) [text/plain]\n",
      "Saving to: ‘dinos.txt.1’\n",
      "\n",
      "100%[======================================>] 19,909      --.-K/s   in 0.01s   \n",
      "\n",
      "2019-07-05 16:05:42 (1.95 MB/s) - ‘dinos.txt.1’ saved [19909/19909]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/artemovae/NLP-seminar-LM/master/dinos.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8C6AXbd1jst_"
   },
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "Input:\n",
    "\n",
    "$x_{1:n} = x_1, x_2, \\ldots, x_n$, $x_i \\in \\mathbb{R}^{d_{in}}$\n",
    "\n",
    "For each input $x_{1:i}$ we get an output $y_i$:\n",
    "\n",
    "$y_i = RNN(x_{1:i})$, $y_i \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "For the whole input sequence $x_{1:n}$:\n",
    "\n",
    "$y_{1:n} = RNN^{*}(x_{1:n})$, $y_i \\in \\mathbb{R}^{d_{out}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVdGJAyXjst_"
   },
   "source": [
    "$R$ is a recursive activation function with two inputs: $x_i$ и $s_{i-1}$ (state vector)\n",
    "\n",
    "$RNN^{*}(x_{1:n}, s_0) = y_{1:n}$\n",
    "\n",
    "$y_i = O(s_i) = g(W^{out}[s_{i} ,x_i] +b)$\n",
    "\n",
    "$s_i = R(s_{i-1}, x_i)$\n",
    "\n",
    "$s_i = R(s_{i-1}, x_i) = g(W^{hid}[s_{i-1} ,x_i] +b)$  -- concatenate $[s_{i-1}, x]$\n",
    "\n",
    "$x_i \\in \\mathbb{R}^{d_{in}}$, $y_i \\in \\mathbb{R}^{ d_{out}}$, $s_i \\in \\mathbb{R}^{d_{hid}}$\n",
    "\n",
    "$W^{hid} \\in \\mathbb{R}^{(d_{in}+d_{out}) \\times d_{hid}}$, $W^{out} \\in \\mathbb{R}^{d_{hid} \\times d_{out}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3kydlaIjsuA"
   },
   "source": [
    "![rnn](https://github.com/enggen/Deep-Learning-Coursera/raw/1407e19c98833d2686a0748db26b594f3102301e/Sequence%20Models/Week1/Dinosaur%20Island%20--%20Character-level%20language%20model/images/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCsqUPwsjsuB"
   },
   "source": [
    "We are going to create an RNN-LM using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVpY2fwfjsuB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3715,
     "status": "ok",
     "timestamp": 1555593496617,
     "user": {
      "displayName": "Ivan Maksimov",
      "photoUrl": "",
      "userId": "09853626855261156129"
     },
     "user_tz": -180
    },
    "id": "cZf1AjTejsuD",
    "outputId": "5a9f7b8b-5d2f-4d1d-e70a-d557059a84f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_size = 50\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47DUTIoWjsuG"
   },
   "source": [
    "**Step 1**. Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "zyqQmDHojsuH"
   },
   "outputs": [],
   "source": [
    "class DinosDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        with open('dinos.txt') as f:\n",
    "            content = f.read().lower()\n",
    "            self.vocab = sorted(set(content)) + ['<', '>']\n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.lines = content.splitlines()\n",
    "        self.ch_to_idx = {c:i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_ch = {i:c for i, c in enumerate(self.vocab)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        line = self.lines[index]\n",
    "        #teacher forcing\n",
    "        x_str = '<' + line \n",
    "        y_str = line + '>' \n",
    "        x = torch.zeros([len(x_str), self.vocab_size], dtype=torch.float)\n",
    "        y = torch.empty(len(x_str), dtype=torch.long)\n",
    "        for i, (x_ch, y_ch) in enumerate(zip(x_str, y_str)):\n",
    "            x[i][self.ch_to_idx[x_ch]] = 1\n",
    "            y[i] = self.ch_to_idx[y_ch]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NG7JFWUjsuJ"
   },
   "outputs": [],
   "source": [
    "trn_ds = DinosDataset()\n",
    "trn_dl = DataLoader(trn_ds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8Wn4caYjsuM",
    "outputId": "cf056485-4854-4e0e-8908-eedbd295cba9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aardonyx'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds.lines[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUVas0h6jsuR",
    "outputId": "57cb673b-0697-4ed8-ce28-3fe5c960c954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '<', 28: '>'}\n"
     ]
    }
   ],
   "source": [
    "print(trn_ds.idx_to_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H41Yi3BPjsuU",
    "outputId": "73a33978-b55d-4a1b-a861-002ff3678fba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNIKB8EPjsuW"
   },
   "outputs": [],
   "source": [
    "x, y = trn_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6K8dF4sjsuY",
    "outputId": "8997a574-c947-445b-fd30-88398867aa31",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 29])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTiuj028jsub",
    "outputId": "06ca9634-d5e9-46e5-bd5f-7035b69e7a42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoRJZCN4jsuf"
   },
   "source": [
    "**Step 2**. Define a model, loss function and optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "QlnFtvOejsuf"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.i2h = nn.Linear(input_size + hidden_size,hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, h_prev, x):\n",
    "        \n",
    "        h = torch.torch.functional.F.tanh(self.dropout(self.i2h(torch.cat((x,h_prev), dim=1))))\n",
    "        result = self.i2o(h)\n",
    "        return h, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oj4E8rCZjsuh"
   },
   "outputs": [],
   "source": [
    "model = RNN(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JptwzGSmjsuj"
   },
   "source": [
    "**Step 3**. Declare a sampling procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ll7NCHwJjsuj"
   },
   "source": [
    "![rnn](https://github.com/enggen/Deep-Learning-Coursera/raw/1407e19c98833d2686a0748db26b594f3102301e/Sequence%20Models/Week1/Dinosaur%20Island%20--%20Character-level%20language%20model/images/dinos3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sidenote: we can use Softmax probabilities to evaluate perplexety, which a standart quality measure for the task\n",
    "\n",
    "$ 2^{{H(p)}}=2^{{-\\sum _{x}p(\\hat(y)^{<i>} == y^{<i>}_{train} )\\log _{2}p(\\hat(y)^{<i>} == y^{<i>}_{train})}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "D1UnWuHAjsuk"
   },
   "outputs": [],
   "source": [
    "def sample(model):\n",
    "    model.eval()\n",
    "    word_size=0\n",
    "    newline_idx = trn_ds.ch_to_idx['>']\n",
    "    with torch.no_grad():\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x = h_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "        start_char_idx = trn_ds.ch_to_idx['<']\n",
    "        indices = [start_char_idx]\n",
    "        x[0, start_char_idx] = 1\n",
    "        predicted_char_idx = start_char_idx\n",
    "        \n",
    "        while predicted_char_idx != newline_idx and word_size != 50:\n",
    "            h_prev, y_pred = model(h_prev, x)\n",
    "            y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
    "            \n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=y_softmax_scores.cpu().numpy().ravel())\n",
    "            indices.append(idx)\n",
    "            \n",
    "            x = (y_pred == y_pred.max(1)[0]).float()\n",
    " \n",
    "            \n",
    "            predicted_char_idx = idx\n",
    "            \n",
    "            word_size += 1\n",
    "        \n",
    "        if word_size == 50:\n",
    "            indices.append(newline_idx)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "LCaYxWsmjsum"
   },
   "outputs": [],
   "source": [
    "def print_sample(sample_idxs):\n",
    "    [print(trn_ds.idx_to_ch[x], end ='') for x in sample_idxs]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgaVjs5ojsuo"
   },
   "source": [
    "**Step 4**. Almost done! Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "pcEvaxsVjsup"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for line_num, (x, y) in enumerate(trn_dl):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for i in range(x.shape[1]):\n",
    "            h_prev, y_pred = model(h_prev, x[:, i])\n",
    "            loss += loss_fn(y_pred, y[:, i])\n",
    "            \n",
    "        if (line_num+1) % 100 == 0:\n",
    "            print_sample(sample(model))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "DDzjhaSfjsuq"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataset='dinos', epochs=1):\n",
    "    for e in range(1, epochs+1):\n",
    "        print('Epoch:{}'.format(e))\n",
    "        train_one_epoch(model, loss_fn, optimizer)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F88CtWHVjsus",
    "outputId": "32565c2c-4b15-4114-b89d-a39b1f69a41c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "<sqa>\n",
      "<mn>\n",
      "<ueoonatao>\n",
      "<pvctssaurus>\n",
      "<tnrusaurhs>\n",
      "<ainrar>\n",
      "<naurus>\n",
      "<ardrus>\n",
      "<ciuros>\n",
      "<saxrosrulus>\n",
      "<gmcrosaurud>\n",
      "<cjlrasausus>\n",
      "<suror>\n",
      "<qnysaurud>\n",
      "<kbmuaosres>\n",
      "\n",
      "Epoch:2\n",
      "<laieooaurus>\n",
      "<puctosaurus>\n",
      "<sauruanuhustur>\n",
      "<agonaratopa>\n",
      "<amvrrgsurus>\n",
      "<rewskrqycaurus>\n",
      "<sltaariaaurus>\n",
      "<tcrwsturus>\n",
      "<dounysaurus>\n",
      "<laltasgvcatoaueopodsausus>\n",
      "<ssrngsausus>\n",
      "<suaruaiod>\n",
      "<gscasnaauros>\n",
      "<tautotaurus>\n",
      "<stiytpauras>\n",
      "\n",
      "Epoch:3\n",
      "<altciurus>\n",
      "<maplunresaptosltt>\n",
      "<llpph>\n",
      "<snossalpaurus>\n",
      "<tceonaonaurus>\n",
      "<hvltasiurus>\n",
      "<kyuroshirus>\n",
      "<scthsaiaurus>\n",
      "<habtaphurus>\n",
      "<sejpshysaurus>\n",
      "<kcbtatesam>\n",
      "<lbkhtrarrus>\n",
      "<pulrusaurus>\n",
      "<throuapatrus>\n",
      "<gtcdupaurus>\n",
      "\n",
      "Epoch:4\n",
      "<apcrus>\n",
      "<crsuoaurus>\n",
      "<lortaaurus>\n",
      "<sesaeopaurus>\n",
      "<ascrusturus>\n",
      "<diurysaurus>\n",
      "<kcatatisaurur>\n",
      "<ctemasap>\n",
      "<oucusaurus>\n",
      "<ztlsiuous>\n",
      "<epcrosaurus>\n",
      "<bselaurus>\n",
      "<terstor>\n",
      "<rnyonosaurus>\n",
      "<scrisaurun>\n",
      "\n",
      "Epoch:5\n",
      "<ciuros>\n",
      "<iyurasaurus>\n",
      "<kyuroshurus>\n",
      "<saurucauras>\n",
      "<selascntaurus>\n",
      "<meotiuourus>\n",
      "<hrahtcrraurus>\n",
      "<lisioasaurus>\n",
      "<surolaurus>\n",
      "<pruaisaurus>\n",
      "<tcconaonaurus>\n",
      "<hvrvlrusaurus>\n",
      "<tprosaurus>\n",
      "<atgubcuuaus>\n",
      "<ghcpansosaurus>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, loss_fn, optimizer, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtkwMfSejsut"
   },
   "source": [
    "### Tasks \n",
    "1. Complete the forward fuction for the RNN model\n",
    "2. Rewrite the sampling function so that pangrams (words that contain each character only ones) are generated\n",
    "\n",
    "### Advanced tasks\n",
    "3. Make yourself a dinosaur nickname: initialize the sampling procedure with your own name\n",
    "4. Implemet a fuction ```get_perplexity``` to compute perplexety. Add more layers and check, whether they affect the perplexety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCFjdgjwjsuu"
   },
   "source": [
    "# Reference\n",
    "\n",
    "1. Sampling in  RNN: https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
    "2. Coursera course (main source): https://github.com/furkanu/deeplearning.ai-pytorch/tree/master/5-%20Sequence%20Models\n",
    "3. Coursera course (main source): https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Dinosaurus%20Island%20--%20Character%20level%20language%20model%20final%20-%20v3.ipynb\n",
    "4. LSTM: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZOEH4tcijst-",
    "8C6AXbd1jst_",
    "qtkwMfSejsut"
   ],
   "name": "Dinosaur Island LM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py3-pytorch] *",
   "language": "python",
   "name": "conda-env-py3-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
